{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"뉴스그룹분류.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMNJVHEYZhUaN5zJ4vmEQmK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NljDZTTshq9_"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"NMrDAv4Bj8S0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('/content/drive/MyDrive/news/train.csv')\n","test = pd.read_csv('/content/drive/MyDrive/news/test.csv')"],"metadata":{"id":"8vVrkfRYjovx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.head()"],"metadata":{"id":"9mA_pbHNkLEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.head()"],"metadata":{"id":"P5WOan9RkUcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp = train.target.unique()\n","temp"],"metadata":{"id":"MP3xz2hMkWDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.sort(temp)"],"metadata":{"id":"Bbu8JFt2kt7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 결측치 존재 여부를 확인해주는 함수\n","def check_missing_col(dataframe):\n","    missing_col = []\n","    counted_missing_col = 0\n","    for i, col in enumerate(dataframe.columns):\n","        missing_values = sum(dataframe[col].isna())\n","        is_missing = True if missing_values >= 1 else False\n","        if is_missing:\n","            counted_missing_col += 1\n","            print(f'결측치가 있는 컬럼은: {col}입니다')\n","            print(f'해당 컬럼에 총 {missing_values}개의 결측치가 존재합니다.')\n","            missing_col.append([col, dataframe[col].dtype])\n","    if counted_missing_col == 0:\n","        print('결측치가 존재하지 않습니다')\n","    return missing_col\n","\n","missing_col = check_missing_col(train)"],"metadata":{"id":"6WlmfX5mkxaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.info()"],"metadata":{"id":"3zGe8tepk3WH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.text"],"metadata":{"id":"sxq318DVlcCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#불필요한 문자 정리\n","import re \n","\n","def clean_text(texts): \n","  corpus = [] \n","  for i in range(0, len(texts)): \n","\n","    review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"\\n\\]\\[\\>]', '',texts[i]) #@%*=()/+ 와 같은 문장부호 제거\n","    review = re.sub(r'\\d+','', review)#숫자 제거\n","    review = review.lower() #소문자 변환\n","    review = re.sub(r'\\s+', ' ', review) #extra space 제거\n","    review = re.sub(r'<[^>]+>','',review) #Html tags 제거\n","    review = re.sub(r'\\s+', ' ', review) #spaces 제거\n","    review = re.sub(r\"^\\s+\", '', review) #space from start 제거\n","    review = re.sub(r'\\s+$', '', review) #space from the end 제거\n","    review = re.sub(r'_', ' ', review) #space from the end 제거\n","    corpus.append(review) \n","  \n","  return corpus"],"metadata":{"id":"sdGnfTGdlhQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp = clean_text(train['text'])\n","train['text'] = temp\n","train.head()"],"metadata":{"id":"YMX84szillUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_count = train['target'].value_counts() #유니크값의 개수 확인\n","for i in range(0,20):\n","    print(f'리벨 {i}인 리뷰 개수:',val_count[i])"],"metadata":{"id":"mcpxGbAjl5Cq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#데이터 시각화\n","%matplotlib inline\n","plt.style.use(\"ggplot\")\n","\n","# 히스토그램 을 사용해서 데이터의 분포를 살펴봅니다.\n","feature = train['target']\n","\n","plt.figure(figsize=(10,7.5)) # 그래프 이미지 크기 설정\n","plt.suptitle(\"Bar Plot\", fontsize=30) # 부제목과 폰트 크기 설정\n","\n","plt.title('label', fontsize=20) # 제목과 폰트 크기 설정\n","temp = feature.value_counts() # feature 변수의 변수별 개수 계산\n","plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5) # 막대그래프 생성\n","plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n","plt.show() # 그래프 나타내기"],"metadata":{"id":"4crDD41aJkGu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["20가지 라벨 모두 300개가 넘고 19이 가장 적음"],"metadata":{"id":"c6dmvojELiiH"}},{"cell_type":"code","source":["str_len_mean = np.mean(train['text'].str.len()) # 리뷰 길이의 평균값 계산\n","print('뉴스의 평균 길이 :',round(str_len_mean,0))"],"metadata":{"id":"k8XWDyGtKlgs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 필터링을 위한 마스크 설정\n","for i in range(0, 20):\n","  globals()['mask_{}'.format(i)] = (train.target == i)\n","\n","# 전체 및 그룹 집합을 설정합니다.\n","df_train = train.text.copy() # 전체 train 데이터\n","\n","for i in range(0, 20):\n","  globals()['df_{}'.format(i)] = train.loc[globals()['mask_{}'.format(i)],:].text # 20가지 라벨 각각에 해당하는 데이터를 df0~19로 할당\n","\n","# 스무가지로 나뉜 집합을 리스트로 묶어줍니다.\n","compare = [df_0, df_1, df_2, df_3, df_4, df_5, \n","           df_6, df_7, df_8, df_9, df_10, df_11,\n","           df_12, df_13, df_14, df_15, df_16, df_17,\n","           df_18, df_19]"],"metadata":{"id":"QA9vtg_zL4yX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_0"],"metadata":{"id":"z_YQi063L-4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 히스토그램 을 사용해서 데이터의 분포를 살펴봅니다.\n","plt.figure(figsize=(40,20))\n","plt.suptitle(\"Histogram: news length\", fontsize=40)\n","name = ['alt.atheism',\n","        'comp.graphics',\n","        'comp.os.ms-windows.misc',\n","        'comp.sys.ibm.pc.hardware',\n","        'comp.sys.mac.hardware',\n","        'comp.windows.x',\n","        'misc.forsale',\n","        'rec.autos',\n","        'rec.motorcycles',\n","        'rec.sport.baseball',\n","        'rec.sport.hockey',\n","        'sci.crypt',\n","        'sci.electronics',\n","        'sci.med',\n","        'sci.space',\n","        'soc.religion.christian',\n","        'talk.politics.guns',\n","        'talk.politics.mideast',\n","        'talk.politics.misc',\n","        'talk.religion.misc'] # 제목으로 사용할 문자열 (라벨의 실제 이름)\n","\n","for i in range(len(compare)):\n","    text = compare[i]\n","    string_len = [len(x) for x in text]    \n","    plt.subplot(5,4,i+1) # 행 개수/ 열 개수/ 해당 그래프 표시 순서\n","    plt.title(name[i], fontsize=20)\n","    plt.axis([0, 50000, 0, 10])  #x축 시작, 끝 / y축 시작, 끝\n","    plt.hist(string_len, alpha=0.5, color='orange') # 히스토그램 생성, alpha: 그래프의 투명도 조절\n","    \n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()"],"metadata":{"id":"E7hNubRUMDG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud, ImageColorGenerator\n","from PIL import Image\n","import nltk # 영어에 대한 전처리\n","import pickle\n","from nltk.corpus import stopwords\n","from os import path\n","nltk.download('all')"],"metadata":{"id":"I-2pr5W1MKtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def df2str(df):\n","    \n","    s = [s for s in df]\n","    document = \"\"\n","    for i in range(len(s)):\n","        document += s[i]\n","    return document"],"metadata":{"id":"dHfV6VLtOOQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 워드 토큰화 :  nltk의 word_tokenize 메소드를 이용하여 토큰화\n","word_tokens = nltk.word_tokenize(df2str(df_0))"],"metadata":{"id":"sSEhf46nOtSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nltk의 pos_tag 메소드를 이용하여 품사분리를 진행\n","# pos_tag()의 입력값으로는 단어의 리스트가 들어가야 한다.\n","tokens_pos = nltk.pos_tag(word_tokens)"],"metadata":{"id":"kUiEyS5QOz9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 명사만 추출\n","# 명사는 NN을 포함하고 있음을 알 수 있음\n","NN_words = []\n","for word, pos in tokens_pos:\n","    if 'NN' in pos:\n","        NN_words.append(word)"],"metadata":{"id":"Bgf_GNZgO5sX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 품사 원형 찾기\n","# nltk에서 제공하는 WordNetLemmatizer() 메소드\n","# 명사의 경우 보통 복수 -> 단수 형태로 변형\n","wlem = nltk.WordNetLemmatizer()\n","lemmatized_words = []\n","for word in NN_words:\n","    new_word = wlem.lemmatize(word)\n","    lemmatized_words.append(new_word)"],"metadata":{"id":"gqlQWukLPCQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 불용어 처리\n","# nltk에서 제공하는 불용어 기본 사전 이용하여 워드클라우드에 사용하지 않을 단어를 제거\n","\n","stopwords_list = stopwords.words('english') #nltk에서 제공하는 불용어사전 이용\n","#print('stopwords: ', stopwords_list)\n","unique_NN_words = set(lemmatized_words)\n","final_NN_words = lemmatized_words\n","\n","# 불용어 제거\n","for word in unique_NN_words:\n","    if word in stopwords_list:\n","        while word in final_NN_words: final_NN_words.remove(word)"],"metadata":{"id":"mc9w3hT_PM7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 빈도분석\n","# 빈도수 상위 10개의 단어를 출력\n","from collections import Counter\n","c = Counter(final_NN_words)\n","k = 10"],"metadata":{"id":"p-4eTH3EPUR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c.most_common(k) # 빈도수 기준 상위 k개 단어 출력"],"metadata":{"id":"e72lGbA4Pg7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#시각화\n","top_10 = c.most_common(k) # 추출한 명사 중 상위 10개\n","keys = [top_10[i][0] for i in range(len(top_10))]\n","values = [top_10[i][1] for i in range(len(top_10))]\n","\n","plt.figure(figsize=(10,7.5))\n","plt.suptitle(\"Bar Plot\", fontsize=30)\n","plt.title('total reviews', fontsize=20)\n","plt.bar(keys, values, width=0.5, color='b', alpha=0.5)\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()"],"metadata":{"id":"6G2Zet2nPiiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 워드클라우드\n","\n","noun_text = ''\n","for word in final_NN_words:\n","    noun_text = noun_text +' '+word\n","\n","wordcloud = WordCloud(max_font_size=50, #가장 큰 폰트 크기 제한\n","                      width=500, #너비\n","                      height=300, #높이\n","                      background_color='white', #배경 색상\n","                      relative_scaling=.2 #상대적인 크기\n","                      ).generate(noun_text)\n","\n","plt.figure(figsize=(20,10))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"yO2URR37Pm6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0번 뉴스 그룹 'alt.atheism'의 키워드가 다음과 같이 나와있음\n","\n","'god', 'people', 'argument', 'time' 등 단어가 유즈넷 뉴스의 atheism (무신론) 그룹에서 많이 언급"],"metadata":{"id":"0ahneUaBP59n"}},{"cell_type":"markdown","source":["\n","*   임베딩(word embedding): 컴퓨터가 계산을 하기 위해서는 먼저 문장을 숫자형 값으로 바꾼 다음 계산\n","*   CountVectorizer: 입력된 문장을 토큰화(Tokenize)하여 토큰의 등장 빈도 벡터로 바꿔주는 기법\n","*  토큰화란 문장를 의미가 있는 최소의 단위로 쪼개는 것\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"83xaFNlSQ542"}},{"cell_type":"code","source":["# CountVectorizer 사용하여 띄어쓰기 기준으로 토큰화\n","from sklearn.feature_extraction.text import CountVectorizer #sklearn 패키지의 CountVectorizer import\n","\n","sample_vectorizer = CountVectorizer() #객체 생성"],"metadata":{"id":"kbHVP9etQO6E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CountVectorizer 의 경우 단어의 출현 빈도를 기준으로 문장을 벡터(=숫자의 집합)로 바꿔준다\n","sample_text1 = [\"hello, my name is dacon and I am a data scientist!\"]\n","\n","sample_vectorizer.fit(sample_text1) #CountVectorizer 학습"],"metadata":{"id":"1UKgoS-kVFW9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sample_vectorizer.vocabulary_) #Vocabulary"],"metadata":{"id":"iYQGpl6nVQnq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sklearn의 CountVectorizer는 \"I\", \"a\" 와 같이 길이가 2 이하인 단어들과, \",\", !\" 같은 특수 문자를 의미가 적다고 판단하여 제외"],"metadata":{"id":"wX1VnnGXVoKv"}},{"cell_type":"code","source":["# sample_vectorizer 를 활용해 벡터로 transform\n","\n","sample_text2 = [\"you are learning dacon data science\"]\n","\n","sample_vector = sample_vectorizer.transform(sample_text2)\n","print(sample_vector.toarray())"],"metadata":{"id":"hRe3bJlpVe0t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["단어들의 출현 빈도로 이루어진 크기 9의 백터가 출력되었습니다. 이 벡터를 BoW 라고 합니다.\n","\n","BoW 란 \"Bag of Words\" 의 약자로 CountVectorizer 로 변환된 단어의 집합을 말합니다\n","\n","특정 단어가 N번 등장했다면, 그 특정 단어가 N개가 들어있습니다.\n","\n","또한 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않습니다.\n","\n","BoW를 만드는 과정은 이렇게 두 가지 과정이 있습니다.\n","\n","(1) 우선 각 단어에 고유한 정수 인덱스를 부여합니다.\n","\n","(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.\n","\n","sample_text2 문장 내에는 \"dacon\", \"data\" 단어가 각 1번 씩 출현했습니다.\n","\n","따라서 \"dacon\" 과 \"data\" 에 해당하는 인덱스의 값이 1로 표시되어 있고, 등장하지 않은 단어들은 0으로 표시되어 있습니다."],"metadata":{"id":"GNyf3WHsWTK6"}},{"cell_type":"code","source":["sample_text3 = [\"you are learning dacon data science with news data\"]\n","\n","sample_vector2 = sample_vectorizer.transform(sample_text3)\n","print(sample_vector2.toarray())"],"metadata":{"id":"aay9NeHCWKch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\"you are learning dacon data science with news data\" 문장에는 \"dacon\" 단어가 1번, \"data\" 단어가 2번 출현했습니다.\n","\n","따라서 \"dacon\" 단어에 해당하는 인덱스에는 1의 값이, \"data\" 에 해당하는 인덱스에는 2의 값이 할당되었습니다.\n","\n","CountVectorizer 는 위와 같이 작동합니다.\n","\n","이해를 위해 CountVectorizer 를 하나의 문장을 사용해 학습 시켰기에 생성된 Vocab과 BoW 의 크기는 모두 9 였습니다.\n","\n","하지만 더 큰 데이터를 사용해 CountVectorizer 를 학습시킨다면 Vocab과 Bow 의 크기는 더욱 증가할 것입니다.\n","\n","이처럼 BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이기 때문에,\n","\n","주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰입니다.\n","\n","즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰입니다."],"metadata":{"id":"4GAgYlEDWpDA"}},{"cell_type":"code","source":["X = train.text #training 데이터에서 문서 추출\n","y = train.target #training 데이터에서 라벨 추출"],"metadata":{"id":"KrmVm0HGXBAZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.target"],"metadata":{"id":"njvWfkGlXNuA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"2nnYsqxBXG3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = CountVectorizer() #countvectorizer 생성\n","vectorizer.fit(X) #countvectorizer 학습\n","X = vectorizer.transform(X) #transform"],"metadata":{"id":"zkCTmqiGWnKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer.inverse_transform(X[0]) #역변환하여 첫번째 문장의 단어들 확인"],"metadata":{"id":"An5fi0elWtJl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["예측하고자 하는 것이 연속적인 값을 갖는 수치 형태가 아닌, 1 단위로 끊어지는 분류 문제이기 때문에\n","\n","category를 분류할 때 사용하는 LogisticRegression을 이용하여 진행"],"metadata":{"id":"6R-J9sn1Xd0K"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression #모델 불러오기\n","model = LogisticRegression(max_iter=500) #객체에 모델 할당\n","model.fit(X, y) #모델 학습"],"metadata":{"id":"LTWPRWtvW5Lq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","#run model\n","y_pred = model.predict(X[0])\n","print('예측 라벨 : ', y_pred)\n","print('실제 라벨 : ', train.target[0])"],"metadata":{"id":"XrcvEk4gXgDY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["X의 첫번째 문장은 10번 카테고리 그룹으로 잘 학습되어 분류된 것을 확인"],"metadata":{"id":"RqbqVkN4Xuss"}},{"cell_type":"code","source":["test_X = test.text #문서 데이터 생성\n","\n","test_X_vect = vectorizer.transform(test_X) #문서 데이터 transform \n","#test 데이터를 대상으로 fit_transform 메소드를 실행하는 것은 test 데이터를 활용해 vectorizer 를 학습 시키는 것으롤 data leakage 에 해당합니다.\n","\n","pred = model.predict(test_X_vect) #test 데이터 예측\n","print(pred)"],"metadata":{"id":"ci7kUwb3XvEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = pd.read_csv(\"/content/drive/MyDrive/news/sample_submission.csv\") #제출용 파일 불러오기\n","submission.head() #제출 파일이 잘 생성되었는지 확인"],"metadata":{"id":"jI2bFJ5bX5Gm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission[\"target\"] = pred #예측 값 넣어주기\n","submission.head() # 데이터가 잘 들어갔는지 확인합니다."],"metadata":{"id":"GPXPNDCmX6na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# submission을 csv 파일로 저장합니다.\n","# index=False란 추가적인 id를 부여할 필요가 없다는 뜻입니다. \n","# 정확한 채점을 위해 꼭 index=False를 넣어주세요.\n","submission.to_csv(\"/content/drive/MyDrive/news/submission.csv\",index=False)"],"metadata":{"id":"gFHT0yu1X9YC"},"execution_count":null,"outputs":[]}]}